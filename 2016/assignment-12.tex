% Created 2016-04-03 Sun 22:02
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage[hidelinks]{hyperref}
\tolerance=1000
\usepackage[utf8]{inputenc}
\usepackage[usenames,dvipsnames]{color}
\usepackage{a4wide}
\usepackage{commath}
\usepackage{amsmath}
\usepackage{marginnote}
\usepackage{enumerate}
\usepackage{listings}
\usepackage{color}
\hypersetup{urlcolor=blue}
\hypersetup{colorlinks,urlcolor=blue}
\setlength{\parskip}{16pt plus 2pt minus 2pt}
\definecolor{codebg}{rgb}{0.96,0.99,0.8}
\DeclareMathOperator{\det}{det}
\author{Oleg Sivokon}
\date{\textit{<2016-03-26 Sat>}}
\title{Assignment 12, Linear Algebra 1}
\hypersetup{
 pdfauthor={Oleg Sivokon},
 pdftitle={Assignment 12, Linear Algebra 1},
 pdfkeywords={Assignment, Linear Algebra},
 pdfsubject={Second asssignment in the course Linear Algebra 1},
 pdfcreator={Emacs 25.0.50.1 (Org mode 8.3beta)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

\definecolor{codebg}{rgb}{0.96,0.99,0.8}
\lstnewenvironment{maxima}{%
  \lstset{backgroundcolor=\color{codebg},
    frame=single,
    framerule=0pt,
    basicstyle=\ttfamily\scriptsize,
    columns=fixed}}{}
}
\makeatletter
\newcommand{\verbatimfont}[1]{\renewcommand{\verbatim@font}{\ttfamily#1}}
\makeatother
\verbatimfont{\small}%
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother
\clearpage

\section{Problems}
\label{sec:orgheadline18}

\subsection{Problem 1}
\label{sec:orgheadline3}
Let \(A\) be a square matrix of order \(3 \times 3\) s.t. \(A^3 = 0\), but
\(A^2 \neq 0\).

\begin{enumerate}
\item Prove that there exists a vector \(\vec{v} \in \mathbb{R}^3\) s.t. \(A\vec{v}
      \neq 0\).
\item Prove that there exits vector \(\vec{v} \in \mathbb{R}^3\) s.t.
\(\{\vec{v}, \vec{v}A, \vec{v}A^2\}\) are the basis of \(\mathbb{R}^3\).
\end{enumerate}

\subsubsection{Answer 1}
\label{sec:orgheadline1}
Notice that \(A\) itself is made of the column vectors, call them \(c_1, c_2,
    c_3\).  All of which are in \(\mathbb{R}^3\).  Suppose, for contradiction, that
there is no vector in \(\vec{v} \in \mathbb{R}^3\) satisfying \(A\vec{v} \neq
    0\).  In particular, none of the \(\vec{c}_1, \vec{c}_2, \vec{c}_3\) satisfies
the above condition.  In other words, \(A\vec{c}_1 = 0\), \(A\vec{c}_2 = 0\),
\(A\vec{c}_3 = 0\).  (where 0 means zero matrix).  On the other hand, a matrix
\((A\vec{c}_1, A\vec{c}_2, A\vec{c}_3) = A^2 \neq 0\).  Contradiction.  Hence,
there exists \(\vec{v} \in \mathbb{R}^3\) s.t. \(A\vec{v} \neq 0\).

\subsubsection{Answer 2}
\label{sec:orgheadline2}
Suppose for contradiction there was no such vector \(\vec{v}\).  This would in
turn imply that all vectors \(\vec{x} \in \mathbb{R}^3\) would be sent by
\(A^2(\vec{x})\) to its kernel, i.e \(\forall \vec{x} \in \mathbb{R}^3:
    A^2(\vec{x}) = [0]\).  But we just proved the opposite in the previous
answer.  Hence, by contradiction, there must exist \(\vec{v}\)
s.t. \(\{\vec{v}, A\vec{v}, A^2\vec{v}\}\) is the span (and hence the basis)
of \(\mathbb{R}^3\).

\subsection{Problem 2}
\label{sec:orgheadline5}
Given square matrices \(A, B, C, D\) of order \(n \times n\) s.t. \(ABCD = I\),
prove that \(ABCD = DABC = CDAB = BCDA = I\).

\subsubsection{Answer 3}
\label{sec:orgheadline4}
The proof is immediate from the definition of inverse: \(XX^{-1} = I\) and
associativity of matrix multiplication.  In other words:

\begin{align*}
  ABCD &= I \iff \\
  A(BCD) &= I \iff \\
  A^{-1} &= BCD \iff \\
  DABC &= I \iff \\
  AB(CD) &= I \iff \\
  (AB)^{-1} &= CD \iff \\
  CDAB &= I \iff \\
  A^{-1}A &= I \iff \\
  I &= BCDA \;.
\end{align*}

\subsection{Problem 3}
\label{sec:orgheadline7}
Let \(A\) be a square matrix of order \(m \times m\), let \(B\) be a matrix of
order \(m \times n\).  Prove in two different ways that if \(A\) is invertible,
then homogeneous systems \(B\vec{x} = 0\) and \(AB\vec{x} = 0\) have the same
solution space.

\subsubsection{Answer 4}
\label{sec:orgheadline6}
\begin{enumerate}
\item Since \(A\) is invertible, we can write:

\begin{align*}
  Bx &= \vec{0} \iff \\
  ABx &= A\vec{0} \iff \\
  ABx &= \vec{0} \\
        &\textit{Alternatively:} \\
  ABx &= \vec{0} \iff \\
  A^{-1}ABx &= A^{-1}\vec{0} \iff \\
  Bx &= \vec{0}\;.
\end{align*}

\item Alternatively, let \(T_A(x)=Ax\) and \(T_B(x) = Bx\) be linear
transformations.  Since \(T_A(x)\) is one-to-one and onto (due to \(A\) being
invertible), then \((T_A \circ T_B)(x)\) has the same kernel space as
\(T_B(x)\).  In other words, \(ABx = Bx\) when \(Bx = 0\).
\end{enumerate}

\subsection{Problem 4}
\label{sec:orgheadline9}
Given matrix \(A\) of a general form:

\begin{align*}
  \begin{bmatrix}
    0      & a_1    & \dots  & 0      & 0 \\
    0      & 0      & a_2    & \dots  & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0      & 0      & 0      & \dots  & a_{n-1} \\
    a_n    & 0      & 0      & \dots  & 0
  \end{bmatrix}
\end{align*}

Prove that it is invertible, show \(A^{-1}\).

\subsubsection{Answer 5}
\label{sec:orgheadline8}
Performing elementary operations: \(R_n \to R_1\) and \(R_k \to R_{k+1}, 1 \leq
    k < n\) gives us diagonal matrix.  This matrix is invertible since it has a
pivot element in each of its columns.

The inverse of \(A\) will, in general look like this:

\begin{align*}
  \begin{bmatrix}
    0             & 0             & \dots  & 0                & \frac{1}{a_n} \\
    \frac{1}{a_1} & 0             & \dots  & 0                & 0 \\
    0             & \frac{1}{a_2} & \dots  & 0                & 0 \\
    \vdots        & \vdots        & \ddots & \vdots           & \vdots \\
    0             & 0             & \dots  & \frac{1}{a_{n-1}} & 0 \\
  \end{bmatrix}
\end{align*}

Notice that for each row of \(A\), we will be matching the column of \(A^{-1}\).
We need to make sure that the only non-zero element of \(A_c\) was matched by
the only non-zero element of \(A^{-1}_r\) (where \(c\) stands for column index
and \(r\) stands for row index).  In order to obtain a diagonal with all ones
(i.e. the identity matrix), we need to also make sure that \(A_{c,i} \times
    A^{-1}_{r,j} = 1\).  In other words, we need to match \(a_1\) with \(\frac{1}{a_1}\),
\(a_2\) with \(\frac{1}{a_2}\), and so on.

\subsection{Problem 5}
\label{sec:orgheadline11}
Let \(A\) and \(B\) be square matrices of the order \(3 \times 3\) s.t. \(B^2A = -2B^3\)
and \(B^3 + AB^2 = 3I\).

Prove that \(A\) and \(B\) are invertible and express \(A^{-1}\) and \(B^{-1}\) in
terms of \(B\).

\subsubsection{Answer 6}
\label{sec:orgheadline10}
Using some matrix algebra we obtain: \(B^{-1} = -\frac{1}{3}B^2\) and \(A^{-1}
    = (-2B)^{-1}\).

\begin{align*}
  B^2A &= -2B^3 \iff \\
  B^2A &= B^2(-2I)B \iff \\
  A &= -2B \\
  &\textit{substituting into second equation:} \\
  B^3 + AB^2 &= 3I \iff \\
  B^3 - 2B^3 &= 3I \iff \\
  -B^3 &= 3I \iff \\
  B(-B^2) &= 3I \iff \\
  B(-\frac{1}{3}B^2) &= I \iff \\
  B^{-1} &= -\frac{1}{3}B^2 \\
  &\textit{$A$ is invertible because it is similar to $B$} \\
  -2B &= (\sqrt{2}I)B(\sqrt{2}I^{-1}) \\
  A^{-1} &= (-2B)^{-1} \;.
\end{align*}

\subsection{Problem 6}
\label{sec:orgheadline14}
\begin{enumerate}
\item Compute the determinant:
\begin{equation*}
  D = \left|
    \begin{array}{lllrll}
      1      & 1      & 1      & \dots  & 1      & 1      \\
      1      & 1      & 1      & \dots  & 1      & 2      \\
      1      & 1      & \dots  & 1      & 3      & 1      \\
      \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
      1      & 1      & n-1    & \dots  & 1      & 1      \\
      1      & n      & 1      & \dots  & 1      & 1
    \end{array}
  \right|
\end{equation*}

\item Prove that given:

\begin{equation*}
  \Delta = \left|
    \begin{array}{llllll}
      a_1    & a^2_1    & \dots  & a^{n-1}_1    & 1 + a^n_1    \\
      a_2    & a^2_2    & \dots  & a^{n-1}_2    & 1 + a^n_2    \\
      \vdots & \vdots  & \vdots & \vdots      & \vdots      \\
      a_{n-1} & a^2_{n-1} & \dots  & a^{n-1}_{n-1} & 1 + a^n_{n-1} \\
      a_n    & a^2_n    & \dots  & a^{n-1}_n    & 1 + a^n_n
    \end{array}
  \right|
\end{equation*}

and

\begin{equation*}
  \Delta_1 = \left|
    \begin{array}{llllll}
      a_1    & a^2_1    & \dots  & a^{n-1}_1    & 1      \\
      a_2    & a^2_2    & \dots  & a^{n-1}_2    & 1      \\
      \vdots & \vdots  & \vdots & \vdots      & \vdots \\
      a_{n-1} & a^2_{n-1} & \dots  & a^{n-1}_{n-1} & 1      \\
      a_n    & a^2_n    & \dots  & a^{n-1}_n    & 1
    \end{array}
  \right|
\end{equation*}

where \(a_1, a_2, \dots, a_n \in \mathbb{R}\).  When \(\Delta = 0\) and
\(\Delta_1 \neq 0\), \(\prod_{i=1}^n a_i = -1\).
\end{enumerate}

\subsubsection{Answer 7}
\label{sec:orgheadline12}
Firs, notice that we can row-reduce this matrix without affecting the value
of the determinant by subtracting the top row from other rows.  Once this is
done, we will swap rows in order to bring this matrix to upper-diagonal
form.  To do this, we will need to swap \(\frac{n}{2}\) times if \(n\) is even
and \(\frac{n-1}{2}\) times when \(n\) is odd.  Thus, we can compute \(D\) using
the formula below:

\begin{align*}
  D &= \begin{cases}
    n!,          &\textbf{if}\; \lfloor \frac{n}{2} \rfloor \equiv 0 \mod 2 \\
    -1\times n!, &\textbf{if}\; \lfloor \frac{n}{2} \rfloor \equiv 1 \mod 2
  \end{cases}
\end{align*}

Note that we get \(n!\) term by multiplying the diagonal entries, the \(-1\)
appears due to the odd number of row swaps.

\subsubsection{Answer 8}
\label{sec:orgheadline13}
Let \(\Delta = \det(A)\), \(\Delta_1 = \det(B)\), \(C = (A_0, A_1, \dots, [a_1^n,
    a_2^n, \dots, a_n^n])\) s.t. \(\abs{C} + \abs{B} = \abs{A}\).  Or, in other
words, \(\abs{C} = -\abs{B} = (-1)^n\abs{B}\).  However, since we know that
the determinant of \(C\) is a multiple of determinant of \(B\), and they differ
in one row, we can conclude that these rows are multiples of each other.  In
other words, \((-1)^n \tiems [1, 1, \dots, 1] = [a^n_1, a^n_2, \dots,
    a^n_n]\).  Hence \(\prod_{i = 1}^n a_i = (-1)^n1 = (-1)^n\).

\subsection{Problem 7}
\label{sec:orgheadline17}
Given antisymmetric matrix \(A \in \mathbb{M}^{3\times 3}\) and any matrix \(B
   \in \mathbb{M}^{3\times 3}\), 

\begin{enumerate}
\item prove \((A^2B)\vec{x} = \vec{0}\) has non-trivial soution.
\item Assume \(B\) is antisymmetric, and both \(A\) and \(B\) are non-zero, is \((A +
      B)^2\) symmetric, invertible?
\end{enumerate}

\subsubsection{Answer 9}
\label{sec:orgheadline15}
Any antisymmetric matrix of odd degre is not invertible since \(\det(A) =
    \det(A^T) = \det(-A^T) = (-1)^n\det(A^T)\).  Thus, in particular, \(A^2B\) is
not invertible (product of invertible and non-invertible matrix is not
invertible).  Non-invertible means free variable, hence infinitely many
solutions.

\subsubsection{Answer 10}
\label{sec:orgheadline16}
The matrix \((A - B)^2\) is not in general invertible or symmetric, or
antisymmetric.  For example, let:

\begin{align*}
  A = \begin{bmatrix}
    0 & 0 & -1 \\
    0 & 0 & 0 \\
    1 & 0 & 0
  \end{bmatrix} \\
  B = \begin{bmatrix}
    0  & 0 & -1 \\
    0  & 0 & 0 \\
    -1 & 0 & 0
  \end{bmatrix}
\end{align*}

then:

\begin{align*}
  (A - B)^2 = \left(\begin{bmatrix}
    0 & 0 & -1 \\
    0 & 0 & 0 \\
    1 & 0 & 0
  \end{bmatrix} -
  \begin{bmatrix}
    0  & 0 & -1 \\
    0  & 0 & 0 \\
    -1 & 0 & 0
  \end{bmatrix} \right)^2 = \\
  \begin{bmatrix}
    0 & 0 & 0 \\
    0 & 0 & 0 \\
    1 & 0 & 0
  \end{bmatrix} \times 
  \begin{bmatrix}
    0 & 0 & 0 \\
    0 & 0 & 0 \\
    1 & 0 & 0
  \end{bmatrix} = 
  \begin{bmatrix}
    0 & 0 & 0 \\
    0 & 0 & 0 \\
    0 & 0 & 0
  \end{bmatrix}
\end{align*}

Symmetric, but not invertible.  While for:

\begin{align*}
  A = \begin{bmatrix}
    0 & 0 & -1 \\
    0 & 0 & 0 \\
    1 & 0 & 0
  \end{bmatrix} \\
  B = \begin{bmatrix}
    -1 & 0  & 0 \\
    0  & -1 & 0 \\
    0  & 0  & -1
  \end{bmatrix}
\end{align*}


\begin{align*}
  (A - B)^2 = \left(\begin{bmatrix}
    0 & 0 & -1 \\
    0 & 0 & 0 \\
    1 & 0 & 0
  \end{bmatrix} -
  \begin{bmatrix}
    -1 & 0  & 0 \\
    0  & -1 & 0 \\
    0  & 0  & -1
  \end{bmatrix} \right)^2 = \\
  \begin{bmatrix}
    1 & 0 & -1 \\
    0 & 1 & 0 \\
    1 & 0 & 1
  \end{bmatrix} \times 
  \begin{bmatrix}
    1 & 0 & -1 \\
    0 & 1 & 0 \\
    1 & 0 & 1
  \end{bmatrix} = 
  \begin{bmatrix}
    0 & 0 & -2 \\
    0 & 1 & 0 \\
    2 & 0 & 0
  \end{bmatrix}
\end{align*}

which isn't symmetric or antisymmetric, but is invertible.
\end{document}