% Created 2016-05-24 Tue 17:45
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage[hidelinks]{hyperref}
\tolerance=1000
\usepackage[utf8]{inputenc}
\usepackage[usenames,dvipsnames]{color}
\usepackage{a4wide}
\usepackage{commath}
\usepackage{amsmath}
\usepackage{marginnote}
\usepackage{enumerate}
\usepackage{listings}
\usepackage{color}
\usepackage{breqn}
\usepackage{flexisym}
\usepackage{mathstyle}
\hypersetup{urlcolor=blue}
\hypersetup{colorlinks,urlcolor=blue}
\setlength{\parskip}{16pt plus 2pt minus 2pt}
\definecolor{codebg}{rgb}{0.96,0.99,0.8}
\DeclareMathOperator{\Sp}{Sp}
\DeclareMathOperator{\cis}{cis}
\author{Oleg Sivokon}
\date{\textit{<2016-05-20 Fri>}}
\title{Assignment 14, Linear Algebra 1}
\hypersetup{
  pdfkeywords={Assignment, Linear Algebra},
  pdfsubject={Third asssignment in the course Linear Algebra 1},
  pdfcreator={Emacs 25.1.50.2 (Org mode 8.2.10)}}
\begin{document}

\maketitle
\tableofcontents


\definecolor{codebg}{rgb}{0.96,0.99,0.8}
\lstnewenvironment{maxima}{%
  \lstset{backgroundcolor=\color{codebg},
    frame=single,
    framerule=0pt,
    basicstyle=\ttfamily\scriptsize,
    columns=fixed}}{}
}
\makeatletter
\newcommand{\verbatimfont}[1]{\renewcommand{\verbatim@font}{\ttfamily#1}}
\makeatother
\verbatimfont{\small}%
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother
\clearpage

\section{Problems}
\label{sec-1}

\subsection{Problem 1}
\label{sec-1-1}
Given $f, g, h$ are functions from $\mathbb{R}$ to $\mathbb{R}$, check that
all of them are linearly independent when:
\begin{enumerate}
\item $f(x) = \sin x$, $g(x) = \cos x$, $h(x) = x \cos x$.
\item $f(x) = x(x - 1)$, $g(x) = x(x - 2)$, $h(x) = (x - 1)(x - 2)$.
\item $f(x) = \sin^2 x$, $g(x) = \cos^2 x$, $h(x) = 3$.
\end{enumerate}

\subsubsection{Answer 1}
\label{sec-1-1-1}
Assuming interval is $(-\infty, \infty)$, using Wronskian determinant:
\begin{align*}
  D = \left|
  \begin{array}{lll}
    \sin x  & \cos x  & x\cos x \\
    \cos x  & -\sin x & \cos x - x \sin x \\
    -\sin x & -\cos x & -\sin x - \sin x - x \cos x
  \end{array}
  \right| = \left|
  \begin{array}{lll}
    \sin x  & \cos x  & x\cos x \\
    \cos x  & -\sin x & \cos x - x \sin x \\
    0       & 0       & -2\sin x
  \end{array}
  \right| = \\
  -2\sin x \times \left|\begin{array}{lll}
    \sin x  & \cos x \\
    \cos x  & -\sin x 
  \end{array}
  \right| = -2 \sin x (-\sin^2 x - \cos^2 x) = 2 \sin x\;.
\end{align*}

Since determinant depends on $x$, it is certanly not zero for all $x$, hence
$f, g, h$ are linearly independent.

\subsubsection{Answer 2}
\label{sec-1-1-2}
Assuming interval is $(-\infty, \infty)$, using Wronskian determinant:
\begin{align*}
  D = \left|
  \begin{array}{lll}
    x^2 - x & x^2 - 2x & x^2 - 3x - 3 \\
    2x - 1  & 2x - 2  & 2x - 3 \\
    2       & 2       & 2
  \end{array}
  \right| = \\
  2(x^2 - x)(2x - 2 - 2x + 3) - 4(x^2 - 2x)(2x - 1 - 2x - 2) - 2(x^2 - 3x - 3) = \\
  2x^2 - 2x - 4x^2 + 8x - 2x^2 + 6x + 6 = \\
  -4x^2 + 12x + 6\;.
\end{align*}

Since determinant depends on $x$, it is certanly not zero for all $x$, hence
$f, g, h$ are linearly independent.

\subsubsection{Answer 3}
\label{sec-1-1-3}
Assuming interval is $(-\infty, \infty)$, $a\sin^2(x) + b\cos^2(x) + 3c = 0$
has a solution when $a = b = 1$ and $c = -\frac{1}{3}$.  Thus $f, g, h$ are
linearly dependent.

\subsection{Problem 2}
\label{sec-1-2}
Given the following subsets of $\mathbb{R}^4$:
\begin{align*}
  U &= \{(x, y, z, t) \in \mathbb{R}^4 \;|\; x - y + z = 0 \land x - y - 2t = 0\} \\
  W &= \Sp\{(1,0,1,1), (0,1,0,-1), (1,0,1,0)\}
\end{align*}


\begin{enumerate}
\item Prove that $U$ and $W$ are subspaces of $\mathbb{R}^4$.
\item Find basis for $U$, $W$ and $U+W$.
\item Find basis for $U \cap W$.
\item Find subspace $T$ of $\mathbb{R}^4$ s.t. $U \oplus T = \mathbb{R}^4$.
\end{enumerate}

\subsubsection{Answer 4}
\label{sec-1-2-1}
$W$ is a subspace of $\mathbb{R}^4$ immediately from definition of span.
$U$ is a subspace because it can be represented by $\Sp\{(1,0,0,0),
    (1,1,-1,-2), (0,-1,1,2), (0,-\frac{1}{2},\frac{1}{2},1)\}$ by noting that $x
    = y - 2t$ and $z = 2t$, which is again, by definition, a subspace of
$\mathbb{R}^4$.

\subsubsection{Answer 5}
\label{sec-1-2-2}
Span of $U$ is also its basis since all vectors in it are linearly
independent.  Span of $W$ contains linearly dependent vectors, $t$ is a
multiple of $z$ and both of them are multiples of linear combination of $x$
and $y$, hence its basis has only two vectors, for example $\{(1,0,0,0),
    (0,1,-1,2)\}$.  We can find the basis of $U + W$ by adjoining both bases
and performing Gaussian elimination:
\begin{align*}
  \begin{bmatrix}
    1 & 0 & 1 & 1 \\
    0 & 1 & 0 & -1 \\ 
    1 & 0 & 1 & 0 \\
    1 & 0 & 0 & 0 \\
    0 & 1 & -1 & 2
  \end{bmatrix} \sim
  \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & 0 & 1 & 1 \\
    0 & 1 & 0 & -1 \\ 
    0 & 0 & 1 & 0 \\
    0 & 1 & -1 & 2
  \end{bmatrix} \sim
  \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & -1 \\ 
    0 & 0 & -1 & 3 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 1 & 1 \\
  \end{bmatrix} \sim
  \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & -1 \\ 
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 3 \\
    0 & 0 & 0 & 1
  \end{bmatrix} \sim
  \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & -1 \\ 
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1 \\
    0 & 0 & 0 & 0
  \end{bmatrix}
\end{align*}

Which is the standard basis for $\mathbb{R}^4$.

\subsubsection{Answer 6}
\label{sec-1-2-3}
A generic vector $\vec{v} \in U \cap W$ must be representable as $\vec{v} =
    a(1,0,1,1) + b(0,1,0,-1) + c(1,0,1,0)$ and $\vec{v} = d(1,0,0,0) +
    e(0,1,-1,2)$.  Equating both parts gives: $a(1,0,1,1) + b(0,1,0,-1) +
    c(1,0,1,0) - d(1,0,0,0) - e(0,1,-1,2) = 0$.  Solving for $(a, b, c, d, e)$
will give us the kernel space of $U \cap W$
\begin{align*}
  \begin{bmatrix}
    1 & 0  & 1 & -1 & 0 \\
    0 & 1  & 0 & 0  & -1 \\ 
    1 & 0  & 1 & 0  & 1 \\
    1 & -1 & 0 & 0  & -2 
  \end{bmatrix} \sim
  \begin{bmatrix}
    1 & 0  & 1  & -1 & 0 \\
    0 & 1  & 0  & 0  & -1 \\ 
    0 & 0  & 0  & 1  & 1 \\
    0 & -1 & -1 & 1  & -2 
  \end{bmatrix} \sim
  \begin{bmatrix}
    1 & 0 & 0  & 0 & -3 \\
    0 & 1 & 0  & 0  & -1 \\ 
    0 & 0 & -1 & 0  & -4 \\ 
    0 & 0 & 0  & 1  & 1
  \end{bmatrix}
\end{align*}

Since the only column that doesn't have a pivot element is the last one, the
basis contains just one vector.  That is any solution to $a(1,0,1,1) +
    b(0,1,0,-1) + c(1,0,1,0) - d(1,0,0,0) - e(0,1,-1,2) = 0$ will give us the
basis of $U \cap W$.
\begin{align*}
  3(1,0,1,1) + 1(0,1,0,-1) - 4(1,0,1,0) + 1(1,0,0,0) - 1(0,1,-1,2) = 0 \\
  \vec{v} = 3(1,0,1,1) + 1(0,1,0,-1) - 4(1,0,1,0) = \\
  (3,0,3,3) + (0,1,0,-1) - (4,0,4,0) = (3-4,1,3-4,3-1) = (-1,1,-1,2)
\end{align*}

Hence, the basis of $U \cap W = \{(-1,1,-1,2)\}$.

\subsubsection{Answer 7}
\label{sec-1-2-4}
One way to find such subspace of $\mathbb{R}^4$ we can simply complete the
two vectors of the basis of $U$ with two vectors from the standard basis to
form an invertible matrix like so:
\begin{align*}
  \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & -1 & 2 \\ 
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1 
  \end{bmatrix}
\end{align*}

This is an upper-triangular matrix, therefor invertible, therefore its row
space spans $\mathbb{R}^4$.  Thus $U \oplus \{(0,0,1,0),(0,0,0,1)\} =
    \mathbb{R}^4$.

\subsection{Problem 3}
\label{sec-1-3}
Let $\vec{v}_1, \vec{v}_2, \dots, \vec{v}_k$ and $\vec{w}$ be vectors in
linear space $V$.  Given $\{\vec{v}_1, \vec{v}_2, \dots, \vec{v}_k\}$ is
linearly independent and that $\vec{w} \not \in \Sp\{\vec{v}_1, \vec{v}_2,
   \dots, \vec{v}_k\}$, prove that $\vec{v_1} \not \in \Sp\{\vec{v}_1 + \vec{w},
   \vec{v}_2 + \vec{w}, \dots, \vec{v}_k + \vec{w}\}$.

\subsubsection{Answer 8}
\label{sec-1-3-1}
 Suppose, for contradiction, $\vec{v}_1 = a(\vec{v}_1 + \vec{w}) +
   b(\vec{v}_2 + \vec{w}) + \dots + c(\vec{v}_k + \vec{w})$ for some constants
$a, b \dots c$.  Then we have $(1 - a)\vec{v}_1 - b\vec{v}_2 - \dots -
   c\vec{v}_k = -(a + b + \dots + c)\vec{w}$.  Notice that this says that
$\vec{w}$ is a linear combination of $\{\vec{v}_1, \vec{v}_2, \dots,
   \vec{v}_k\}$, which is a contradiction to the given.  Hence, $\vec{v}_1 \not
   \in \Sp\{\vec{v}_1 + \vec{w}, \vec{v}_2 + \vec{w}, \dots, \vec{v}_k +
   \vec{w}\}$.

\subsection{Problem 4}
\label{sec-1-4}
Let $U$ and $W$ be distinct linear subspaces of $\mathbb{R}^4$ of
dimension 3.  Suppose $(2, 1, 0, 1), (1, 0, 1, 1) \in U \cap W$, what is the
dimension of $U + W$?

\subsubsection{Answer 9}
\label{sec-1-4-1}
Spans of both $U$ and $W$ have a vector that the other doesn't have
(otherwise they would be the same subspace).  Since the span of their
intersection has two distinct vectors, it means that their sum must contain
all linear combinations of the vectors in the span of intersection and the
remaining distinct vector.  This leaves us the only option of $\dim(U+W) =
    4$.

\subsection{Problem 5}
\label{sec-1-5}
Let $A$ and $B$ be square matrices of size $n$, $n \geq 2$.  Suppose $A$ and
$B$ are of rank 1, 
\begin{enumerate}
\item what are the possible ranks of $A + B$?
\item What is the possible rank of $A + B$ when they both are of rank 2?
\item Prove that it is possible to write any matrix of rank 2 as a sum of
two matrices of rank 1.
\end{enumerate}

\subsubsection{Answer 10}
\label{sec-1-5-1}
It can be either zero, one or two.
\begin{enumerate}
\item It can be zero when $A = -B$.  Since this is the zero matrix, its rank is 0.
\item It can be one when, for example, $A = B$, since it would imply $A + B =
       2A$ and rank is preserved under scalar multiplication.
\item For matrix to have rank equal to one it means that all of its non-zero
values are concentrated in either the same row, or the same column.
Whenever a matrix has two or more rows (or columns), where one of the
rows (or columns) is the zero vector, it is possible to find another
matrix, which has in the corresponding position its non-zero entries.
Thus, the rank can be also 2, but no more than that, since another matrix
will necessarily have at most one non-zero row.
\end{enumerate}

\subsubsection{Answer 11}
\label{sec-1-5-2}
Similarly, for matrices of rank 2, we can have rank at leas 0 or at most 4,
or anything in between.
\begin{enumerate}
\item Zeroth rank will result from adding $A$ and $B$ s.t. $A = -B$.
\item Rank of one will result from matrices s.t. the entries of $A$ are
$a_{i,j}$, the entries of $B$ are $b_{i,j}$ and $\forall (0 \geq i \geq n,
       0 \geq j \geq n): a_{i, j} = -b_{i, j}$, unless $i = k$ and $j = m$ (some
constants).  In other words, when all but one entries in these two
matrices are additive inverses of each other except for one entry.
\item The rank will be two when, for example, $A = B$ for the same reason as
above.
\item Example of rank three sum:
\begin{align*}
  \begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 0
  \end{bmatrix} +
  \begin{bmatrix}
    0 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1
  \end{bmatrix} = 
  \begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1
  \end{bmatrix}
\end{align*}

\item Rank can similarly be four:
\begin{align*}
  \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0
  \end{bmatrix} +
  \begin{bmatrix}
    0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1
  \end{bmatrix} = 
  \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1
  \end{bmatrix}
\end{align*}

But it cannot be more than four since both $A$ and $B$ will have at most
two non-zero rows (or columns), which is only enough to produce four
pivot elements in the total.  Since rank is equal to the dimension of
column (or row) span of the matrix, and the dimesnion of these spaces is
determined by the number of pivots, it is thus impossible to have rank
higher than four when adding $A$ and $B$.
\end{enumerate}

\subsubsection{Answer 12}
\label{sec-1-5-3}
Let $C$ be arbitrary matrix of rank 2.  Since rank is preserved under
multiplication with fully-ranked matrix, we can perform Gaussian elimination
on $C$ to obtain a general-form matrix $C'$ lookin like this:
\begin{align*}
  C' &= \begin{bmatrix}
    1      & \dots  & \dots \\
    0      & 1      & \dots \\
    \vdots & \vdots & \ddots
  \end{bmatrix}
\end{align*}

Which, clearly, can be decomposed into two matrices of rank 1.  From
distributivity of matrix multiplication over addition we can show that by
multiplying by elementary matrices we can restore the obtained sum to match
$C$ (the original matrix).  In other words: $C \times c_1E \times c_2E
    \times \dots \times c_nE = C'$, where $Es$ are some elementary matrices and
$c_i$ are some constants, hence $(A + B) \times c_1E \times c_2E \times
    \dots \times c_nE = C$, provided $A + B = C'$.

\subsection{Problem 6}
\label{sec-1-6}
Given bases $B = (\vec{u}_1, \vec{u}_2, \vec{u}_3)$ and $C = (\vec{v}_1,
   \vec{v}_2, \vec{v}_3)$ both in $\mathbb{R}^3$ s.t.
\begin{align*}
  \vec{u}_1 &= (2,1,1) \\
  \vec{u}_2 &= (2,-1,1) \\
  \vec{u}_3 &= (1,2,1) \\
  \vec{v}_1 &= (3,1,-5) \\
  \vec{v}_2 &= (1,1,-3) \\
  \vec{v}_3 &= (-1,0,2)
\end{align*}

\begin{enumerate}
\item Write the matrix of change of basis from $B$ to $C$ and its inverse.
\item Compute the coordinate vector $[\vec{w}]_B$ where $\vec{w} = (-5,8,-5)$.
\item Similarly, compute $[\vec{w}]_C$.
\end{enumerate}

\subsubsection{Answer 13}
\label{sec-1-6-1}
The change of basis matrix from $B$ to $C$ can be found as follow:
\begin{enumerate}
\item Undo the transformation from the standard basis to basis $B$ by
multiplying the coordinates vector by $B^{-1}$.
\item Multiply by $C$ to perform transformation.
\item Multiply by $B$ to represent the resulting coordinates with respect to
$B$.
\end{enumerate}

Or, in other words, $T_{B\to C} = C^{-1}BC$:
\begin{align*}
  \begin{bmatrix}
    3  & 1  & -1 \\
    1  & 1  & 0 \\
    -5 & -3 & 2
  \end{bmatrix}^{-1} \times
  \begin{bmatrix}
    2 & 2  & 1 \\
    1 & -1 & 2 \\
    1 & 1  & 1
  \end{bmatrix} \times
  \begin{bmatrix}
    3  & 1  & -1 \\
    1  & 1  & 0 \\
    -5 & -3 & 2
  \end{bmatrix} = \\
  \begin{bmatrix}
    3  & 1  & -1 \\
    1  & 1  & 0 \\
    -5 & -3 & 2
  \end{bmatrix}^{-1} \times
  \begin{bmatrix}
    6 + 1 - 1 & 6 - 1 - 1 & 3 + 2 - 1 \\
    2 + 1 + 0 & 2 - 1 + 0 & 1 + 2 + 0 \\
    -10 - 3 + 2 & -10 + 3 + 2 & -5 - 6 + 3
  \end{bmatrix} = \\
  \begin{bmatrix}
    3  & 1  & -1 \\
    1  & 1  & 0 \\
    -5 & -3 & 2
  \end{bmatrix}^{-1} \times
  \begin{bmatrix}
    6 & 4 & 4 \\
    3 & 1 & 3 \\
    -11 & -5 & -8
  \end{bmatrix}
\end{align*}

Now, let's find $C^{-1}$:
\begin{align*}
  \begin{bmatrix}
    3  & 1  & -1 & 1 & 0 & 0 \\
    1  & 1  & 0  & 0 & 1 & 0 \\
    -5 & -3 & 2  & 0 & 0 & 1
  \end{bmatrix} \sim
  \begin{bmatrix}
    1 & 1  & 0  & 0 & 1  & 0 \\
    0 & -2 & -1 & 1 & -3 & 0 \\
    0 & 2  & 2  & 0 & 5  & 1
  \end{bmatrix} \sim \\
  \begin{bmatrix}
    1 & 1 & 0           & 0            & 1           & 0 \\
    0 & 1 & \frac{1}{2} & -\frac{1}{2} & \frac{3}{2} & 0 \\
    0 & 0 & 1           & 1            & 2           & 1
  \end{bmatrix} \sim 
  \begin{bmatrix}
    1 & 0 & 0 & 1  & -\frac{1}{2} & \frac{1}{2} \\
    0 & 1 & 0 & -1 & \frac{1}{2}  & -\frac{1}{2} \\
    0 & 0 & 1 & 1  & 2            & 1
  \end{bmatrix}
\end{align*}

finally:
\begin{align*}
  \begin{bmatrix}
    3  & 1  & -1 \\
    1  & 1  & 0 \\
    -5 & -3 & 2
  \end{bmatrix}^{-1} \times
  \begin{bmatrix}
    6   & 4  & 4 \\
    3   & 1  & 3 \\
    -11 & -5 & -8
  \end{bmatrix} = 
  \begin{bmatrix}
    1  & -\frac{1}{2} & \frac{1}{2} \\
    -1 & \frac{1}{2}  & -\frac{1}{2} \\
    1  & 2            & 1
  \end{bmatrix} \times
  \begin{bmatrix}
    6   & 4  & 4 \\
    3   & 1  & 3 \\
    -11 & -5 & -8
  \end{bmatrix} = \\
  \begin{bmatrix}
    6 - \frac{3}{2} - \frac{11}{2}  & 4 - \frac{1}{2} - \frac{5}{2}  & 4 - \frac{3}{2} - 4 \\
    -6 + \frac{3}{2} + \frac{11}{2} & -4 + \frac{1}{2} + \frac{5}{2} & -4 + \frac{3}{2} + 4 \\
    6 + 6 - 11                      & 4 + 2 - 5                      & 4 + 6 - 8
  \end{bmatrix} = 
  \begin{bmatrix}
    -1 & 1 & \frac{3}{2} \\
    1  & 1 & \frac{3}{2} \\
    1  & 1 & 2
  \end{bmatrix}
\end{align*}

\subsubsection{Answer 14}
\label{sec-1-6-2}
$[\vec{w}]_B$ is just $B^{-1}w$.  First, compute $B^{-1}$:
\begin{align*}
  \begin{bmatrix}
    2 & 2  & 1 & 1 & 0 & 0 \\
    1 & -1 & 2 & 0 & 1 & 0 \\
    1 & 1  & 1 & 0 & 0 & 1
  \end{bmatrix} \sim
  \begin{bmatrix}
    1 & 1  & 1  & 0 & 0 & 1 \\
    0 & -2 & 1  & 0 & 1 & -1 \\
    0 & 0  & -1 & 1 & 0 & -2
  \end{bmatrix} \sim
  \begin{bmatrix}
    1 & 1 & 1            & 0  & 0            & 1 \\
    0 & 1 & -\frac{1}{2} & 0  & -\frac{1}{2} & \frac{1}{2} \\
    0 & 0 & 1            & -1 & 0            & 2
  \end{bmatrix} \sim \\
  \begin{bmatrix}
    1 & 1 & 0 & 1            & 0            & -1 \\
    0 & 1 & 0 & -\frac{1}{2} & -\frac{1}{2} & \frac{3}{2} \\
    0 & 0 & 1 & -1           & 0            & 2
  \end{bmatrix} \sim
  \begin{bmatrix}
    1 & 0 & 0 & \frac{3}{2}  & \frac{1}{2}  & -\frac{5}{2} \\
    0 & 1 & 0 & -\frac{1}{2} & -\frac{1}{2} & \frac{3}{2} \\
    0 & 0 & 1 & -1           & 0            & 2
  \end{bmatrix} \\
  \textit{Hence }B^{-1} = 
  \begin{bmatrix}
    \frac{3}{2}  & \frac{1}{2}  & -\frac{5}{2} \\
    -\frac{1}{2} & -\frac{1}{2} & \frac{3}{2} \\
    -1           & 0            & 2
  \end{bmatrix}
\end{align*}

Thus, $[\vec{w}]_B$ can be computed via:
\begin{align*}
  \begin{bmatrix}
    \frac{3}{2}  & \frac{1}{2}  & -\frac{5}{2} \\
    -\frac{1}{2} & -\frac{1}{2} & \frac{3}{2} \\
    -1           & 0            & 2
  \end{bmatrix} \times 
  \begin{bmatrix}
    -5 \\
    8 \\
    -5
  \end{bmatrix} =
  \begin{bmatrix}
    -\frac{15}{2} + 4 + \frac{25}{2} \\
    \frac{5}{2} - 4 - \frac{15}{2} \\
    5 - 10
  \end{bmatrix} = 
  \begin{bmatrix}
    9 \\
    1 \\
    -5
  \end{bmatrix}
\end{align*}

\subsubsection{Answer 15}
\label{sec-1-6-3}
Similarly, $[\vec{w}]_C$ is just $C^{-1}w$ (we've already computed $C^{-1}$
in the previous answer.)
\begin{align*}
  \begin{bmatrix}
    1  & -\frac{1}{2} & \frac{1}{2} \\
    -1 & \frac{1}{2}  & -\frac{1}{2} \\
    1  & 2            & 1
  \end{bmatrix} \times
  \begin{bmatrix}
    -5 \\
    8 \\
    -5
  \end{bmatrix} =
  \begin{bmatrix}
    -5 - 4 - \frac{5}{2} \\
    5  + 4 + \frac{5}{2} \\
    -5 + 16 - 5
  \end{bmatrix} =
  \begin{bmatrix}
    -\frac{23}{2} \\
    \frac{23}{2} \\
    6
  \end{bmatrix}
\end{align*}
% Emacs 25.1.50.2 (Org mode 8.2.10)
\end{document}