# -*- fill-column: 80; org-confirm-babel-evaluate: nil -*-

#+TITLE:     Assignment 14, Linear Algebra 1
#+AUTHOR:    Oleg Sivokon
#+EMAIL:     olegsivokon@gmail.com
#+DATE:      <2016-05-20 Fri>
#+DESCRIPTION: Third asssignment in the course Linear Algebra 1
#+KEYWORDS: Assignment, Linear Algebra
#+LANGUAGE: en
#+LaTeX_CLASS: article
#+LATEX_HEADER: \usepackage[usenames,dvipsnames]{color}
#+LATEX_HEADER: \usepackage{a4wide}
#+LATEX_HEADER: \usepackage{commath}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{marginnote}
#+LATEX_HEADER: \usepackage{enumerate}
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage{breqn}
#+LATEX_HEADER: \usepackage{flexisym}
#+LATEX_HEADER: \usepackage{mathstyle}
#+LATEX_HEADER: \hypersetup{urlcolor=blue}
#+LATEX_HEADER: \hypersetup{colorlinks,urlcolor=blue}
#+LATEX_HEADER: \setlength{\parskip}{16pt plus 2pt minus 2pt}
#+LATEX_HEADER: \definecolor{codebg}{rgb}{0.96,0.99,0.8}
#+LATEX_HEADER: \DeclareMathOperator{\Sp}{Sp}
#+LATEX_HEADER: \DeclareMathOperator{\cis}{cis}

#+BEGIN_SRC emacs-lisp :exports none
  (setq org-latex-pdf-process
          '("latexmk -pdflatex='pdflatex -shell-escape -interaction nonstopmode' -pdf -f %f")
          ;; org-latex-listings t
          org-src-fontify-natively t
          ;; org-latex-custom-lang-environments '((maxima "maxima"))
          ;; org-listings-escape-inside '("(*@" . "@*)")
          ;; org-babel-latex-htlatex "htlatex"
          )

    (defmacro by-backend (&rest body)
      `(progn
         (cl-case org-export-current-backend ,@body)))
#+END_SRC

#+RESULTS:
: by-backend

#+BEGIN_LATEX
\definecolor{codebg}{rgb}{0.96,0.99,0.8}
\lstnewenvironment{maxima}{%
  \lstset{backgroundcolor=\color{codebg},
    frame=single,
    framerule=0pt,
    basicstyle=\ttfamily\scriptsize,
    columns=fixed}}{}
}
\makeatletter
\newcommand{\verbatimfont}[1]{\renewcommand{\verbatim@font}{\ttfamily#1}}
\makeatother
\verbatimfont{\small}%
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother
\clearpage
#+END_LATEX

* Problems

** Problem 1
   Given $f, g, h$ are functions from $\mathbb{R}$ to $\mathbb{R}$, check that
   all of them are linearly independent when:
   1. $f(x) = \sin x$, $g(x) = \cos x$, $h(x) = x \cos x$.
   2. $f(x) = x(x - 1)$, $g(x) = x(x - 2)$, $h(x) = (x - 1)(x - 2)$.
   3. $f(x) = \sin^2 x$, $g(x) = \cos^2 x$, $h(x) = 3$.

*** Answer 1
    Assuming interval is $(-\infty, \infty)$, using Wronskian determinant:
    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{align*}
        D = \left|
        \begin{array}{lll}
          \sin x  & \cos x  & x\cos x \\
          \cos x  & -\sin x & \cos x - x \sin x \\
          -\sin x & -\cos x & -\sin x - \sin x - x \cos x
        \end{array}
        \right| = \left|
        \begin{array}{lll}
          \sin x  & \cos x  & x\cos x \\
          \cos x  & -\sin x & \cos x - x \sin x \\
          0       & 0       & -2\sin x
        \end{array}
        \right| = \\
        -2\sin x \times \left|\begin{array}{lll}
          \sin x  & \cos x \\
          \cos x  & -\sin x 
        \end{array}
        \right| = -2 \sin x (-\sin^2 x - \cos^2 x) = 2 \sin x\;.
      \end{align*}
    #+END_SRC
    Since determinant depends on $x$, it is certanly not zero for all $x$, hence
    $f, g, h$ are linearly independent.

*** Answer 2
    Assuming interval is $(-\infty, \infty)$, using Wronskian determinant:
    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{align*}
        D = \left|
        \begin{array}{lll}
          x^2 - x & x^2 - 2x & x^2 - 3x - 3 \\
          2x - 1  & 2x - 2  & 2x - 3 \\
          2       & 2       & 2
        \end{array}
        \right| = \\
        2(x^2 - x)(2x - 2 - 2x + 3) - 4(x^2 - 2x)(2x - 1 - 2x - 2) - 2(x^2 - 3x - 3) = \\
        2x^2 - 2x - 4x^2 + 8x - 2x^2 + 6x + 6 = \\
        -4x^2 + 12x + 6\;.
      \end{align*}
    #+END_SRC
    Since determinant depends on $x$, it is certanly not zero for all $x$, hence
    $f, g, h$ are linearly independent.

*** Answer 3
    Assuming interval is $(-\infty, \infty)$, $a\sin^2(x) + b\cos^2(x) + 3c = 0$
    has a solution when $a = b = 1$ and $c = -\frac{1}{3}$.  Thus $f, g, h$ are
    linearly dependent.

** Problem 2
   Given the following subsets of $\mathbb{R}^4$:
   #+HEADER: :exports results
   #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
   #+BEGIN_SRC latex
     \begin{align*}
       U &= \{(x, y, z, t) \in \mathbb{R}^4 \;|\; x - y + z = 0 \land x - y - 2t = 0\} \\
       W &= \Sp\{(1,0,1,1), (0,1,0,-1), (1,0,1,0)\}
     \end{align*}
   #+END_SRC
   
   1. Prove that $U$ and $W$ are subspaces of $\mathbb{R}^4$.
   2. Find basis for $U$, $W$ and $U+W$.
   3. Find basis for $U \cap W$.
   4. Find subspace $T$ of $\mathbb{R}^4$ s.t. $U \oplus T = \mathbb{R}^4$.

*** Answer 4
    $W$ is a subspace of $\mathbb{R}^4$ immediately from definition of span.
    $U$ is a subspace because it can be represented by $\Sp\{(1,0,0,0),
    (1,1,-1,-2), (0,-1,1,2), (0,-\frac{1}{2},\frac{1}{2},1)\}$ by noting that $x
    = y - 2t$ and $z = 2t$, which is again, by definition, a subspace of
    $\mathbb{R}^4$.

*** Answer 5
    Span of $U$ is also its basis since all vectors in it are linearly
    independent.  Span of $W$ contains linearly dependent vectors, $t$ is a
    multiple of $z$ and both of them are multiples of linear combination of $x$
    and $y$, hence its basis has only two vectors, for example $\{(1,0,0,0),
    (0,1,-1,2)\}$.  We can find the basis of $U + W$ by adjoining both bases
    and performing Gaussian elimination:
    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{align*}
        \begin{bmatrix}
          1 & 0 & 1 & 1 \\
          0 & 1 & 0 & -1 \\ 
          1 & 0 & 1 & 0 \\
          1 & 0 & 0 & 0 \\
          0 & 1 & -1 & 2
        \end{bmatrix} \sim
        \begin{bmatrix}
          1 & 0 & 0 & 0 \\
          0 & 0 & 1 & 1 \\
          0 & 1 & 0 & -1 \\ 
          0 & 0 & 1 & 0 \\
          0 & 1 & -1 & 2
        \end{bmatrix} \sim
        \begin{bmatrix}
          1 & 0 & 0 & 0 \\
          0 & 1 & 0 & -1 \\ 
          0 & 0 & -1 & 3 \\
          0 & 0 & 1 & 0 \\
          0 & 0 & 1 & 1 \\
        \end{bmatrix} \sim
        \begin{bmatrix}
          1 & 0 & 0 & 0 \\
          0 & 1 & 0 & -1 \\ 
          0 & 0 & 1 & 0 \\
          0 & 0 & 0 & 3 \\
          0 & 0 & 0 & 1
        \end{bmatrix} \sim
        \begin{bmatrix}
          1 & 0 & 0 & 0 \\
          0 & 1 & 0 & -1 \\ 
          0 & 0 & 1 & 0 \\
          0 & 0 & 0 & 1 \\
          0 & 0 & 0 & 0
        \end{bmatrix}
      \end{align*}
    #+END_SRC
    Which is the standard basis for $\mathbb{R}^4$.

*** Answer 6
    A generic vector $\vec{v} \in U \cap W$ must be representable as $\vec{v} =
    a(1,0,1,1) + b(0,1,0,-1) + c(1,0,1,0)$ and $\vec{v} = d(1,0,0,0) +
    e(0,1,-1,2)$.  Equating both parts gives: $a(1,0,1,1) + b(0,1,0,-1) +
    c(1,0,1,0) - d(1,0,0,0) - e(0,1,-1,2) = 0$.  Solving for $(a, b, c, d, e)$
    will give us the kernel space of $U \cap W$
    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{align*}
        \begin{bmatrix}
          1 & 0  & 1 & -1 & 0 \\
          0 & 1  & 0 & 0  & -1 \\ 
          1 & 0  & 1 & 0  & 1 \\
          1 & -1 & 0 & 0  & -2 
        \end{bmatrix} \sim
        \begin{bmatrix}
          1 & 0  & 1  & -1 & 0 \\
          0 & 1  & 0  & 0  & -1 \\ 
          0 & 0  & 0  & 1  & 1 \\
          0 & -1 & -1 & 1  & -2 
        \end{bmatrix} \sim
        \begin{bmatrix}
          1 & 0 & 0  & 0 & -3 \\
          0 & 1 & 0  & 0  & -1 \\ 
          0 & 0 & -1 & 0  & -4 \\ 
          0 & 0 & 0  & 1  & 1
        \end{bmatrix}
      \end{align*}
    #+END_SRC
    Since the only column that doesn't have a pivot element is the last one, the
    basis contains just one vector.  That is any solution to $a(1,0,1,1) +
    b(0,1,0,-1) + c(1,0,1,0) - d(1,0,0,0) - e(0,1,-1,2) = 0$ will give us the
    basis of $U \cap W$.
    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{align*}
        3(1,0,1,1) + 1(0,1,0,-1) - 4(1,0,1,0) + 1(1,0,0,0) - 1(0,1,-1,2) = 0 \\
        \vec{v} = 3(1,0,1,1) + 1(0,1,0,-1) - 4(1,0,1,0) = \\
        (3,0,3,3) + (0,1,0,-1) - (4,0,4,0) = (3-4,1,3-4,3-1) = (-1,1,-1,2)
      \end{align*}
    #+END_SRC
    Hence, the basis of $U \cap W = \{(-1,1,-1,2)\}$.

*** Answer 7
    One way to find such subspace of $\mathbb{R}^4$ we can simply complete the
    two vectors of the basis of $U$ with two vectors from the standard basis to
    form an invertible matrix like so:
    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{align*}
        \begin{bmatrix}
          1 & 0 & 0 & 0 \\
          0 & 1 & -1 & 2 \\ 
          0 & 0 & 1 & 0 \\
          0 & 0 & 0 & 1 
        \end{bmatrix}
      \end{align*}
    #+END_SRC
    This is an upper-triangular matrix, therefor invertible, therefore its row
    space spans $\mathbb{R}^4$.  Thus $U \oplus \{(0,0,1,0),(0,0,0,1)\} =
    \mathbb{R}^4$.

** Problem 3
   Let $\vec{v}_1, \vec{v}_2, \dots, \vec{v}_k$ and $\vec{w}$ be vectors in
   linear space $V$.  Given $\{\vec{v}_1, \vec{v}_2, \dots, \vec{v}_k\}$ is
   linearly independent and that $\vec{w} \not \in \Sp\{\vec{v}_1, \vec{v}_2,
   \dots, \vec{v}_k\}$, prove that $\vec{v_1} \not \in \Sp\{\vec{v}_1 + \vec{w},
   \vec{v}_2 + \vec{w}, \dots, \vec{v}_k + \vec{w}\}$.

*** Answer 8
    Suppose, for contradiction, $\vec{v}_1 = a(\vec{v}_1 + \vec{w}) +
   b(\vec{v}_2 + \vec{w}) + \dots + c(\vec{v}_k + \vec{w})$ for some constants
   $a, b \dots c$.  Then we have $(1 - a)\vec{v}_1 - b\vec{v}_2 - \dots -
   c\vec{v}_k = -(a + b + \dots + c)\vec{w}$.  Notice that this says that
   $\vec{w}$ is a linear combination of $\{\vec{v}_1, \vec{v}_2, \dots,
   \vec{v}_k\}$, which is a contradiction to the given.  Hence, $\vec{v}_1 \not
   \in \Sp\{\vec{v}_1 + \vec{w}, \vec{v}_2 + \vec{w}, \dots, \vec{v}_k +
   \vec{w}\}$.

** Problem 4
   Let $U$ and $W$ be distinct linear subspaces of $\mathbb{R}^4$ of
   dimension 3.  Suppose $(2, 1, 0, 1), (1, 0, 1, 1) \in U \cap W$, what is the
   dimension of $U + W$?

*** Answer 9
    Spans of both $U$ and $W$ have a vector that the other doesn't have
    (otherwise they would be the same subspace).  Since the span of their
    intersection has two distinct vectors, it means that their sum must contain
    all linear combinations of the vectors in the span of intersection and the
    remaining distinct vector.  This leaves us the only option of $\dim(U+W) =
    4$.

** Problem 5
   Let $A$ and $B$ be square matrices of size $n$, $n \geq 2$.  Suppose $A$ and
   $B$ are of rank 1, 
   1. what are the possible ranks of $A + B$?
   2. What is the possible rank of $A + B$ when they both are of rank 2?
   3. Prove that it is possible to write any matrix of rank 2 as a sum of
      two matrices of rank 1.

*** Answer 10
    It can be either zero, one or two.
    1. It can be zero when $A = -B$.  Since this is the zero matrix, its rank is 0.
    2. It can be one when, for example, $A = B$, since it would imply $A + B =
       2A$ and rank is preserved under scalar multiplication.
    3. For matrix to have rank equal to one it means that all of its non-zero
       values are concentrated in either the same row, or the same column.
       Whenever a matrix has two or more rows (or columns), where one of the
       rows (or columns) is the zero vector, it is possible to find another
       matrix, which has in the corresponding position its non-zero entries.
       Thus, the rank can be also 2, but no more than that, since another matrix
       will necessarily have at most one non-zero row.

*** Answer 11
    Similarly, for matrices of rank 2, we can have rank at leas 0 or at most 4,
    or anything in between.
    1. Zeroth rank will result from adding $A$ and $B$ s.t. $A = -B$.
    2. Rank of one will result from matrices s.t. the entries of $A$ are
       $a_{i,j}$, the entries of $B$ are $b_{i,j}$ and $\forall (0 \geq i \geq n,
       0 \geq j \geq n): a_{i, j} = -b_{i, j}$, unless $i = k$ and $j = m$ (some
       constants).  In other words, when all but one entries in these two
       matrices are additive inverses of each other except for one entry.
    3. The rank will be two when, for example, $A = B$ for the same reason as
       above.
    4. Example of rank three sum:
       #+HEADER: :exports results
       #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
       #+BEGIN_SRC latex
         \begin{align*}
           \begin{bmatrix}
             1 & 0 & 0 \\
             0 & 1 & 0 \\
             0 & 0 & 0
           \end{bmatrix} +
           \begin{bmatrix}
             0 & 0 & 0 \\
             0 & 1 & 0 \\
             0 & 0 & 1
           \end{bmatrix} = 
           \begin{bmatrix}
             1 & 0 & 0 \\
             0 & 1 & 0 \\
             0 & 0 & 1
           \end{bmatrix}
         \end{align*}
       #+END_SRC
    5. Rank can similarly be four:
       #+HEADER: :exports results
       #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
       #+BEGIN_SRC latex
         \begin{align*}
           \begin{bmatrix}
             1 & 0 & 0 & 0 \\
             0 & 1 & 0 & 0 \\
             0 & 0 & 0 & 0 \\
             0 & 0 & 0 & 0
           \end{bmatrix} +
           \begin{bmatrix}
             0 & 0 & 0 & 0 \\
             0 & 0 & 0 & 0 \\
             0 & 0 & 1 & 0 \\
             0 & 0 & 0 & 1
           \end{bmatrix} = 
           \begin{bmatrix}
             1 & 0 & 0 & 0 \\
             0 & 1 & 0 & 0 \\
             0 & 0 & 1 & 0 \\
             0 & 0 & 0 & 1
           \end{bmatrix}
         \end{align*}
       #+END_SRC
       But it cannot be more than four since both $A$ and $B$ will have at most
       two non-zero rows (or columns), which is only enough to produce four
       pivot elements in the total.  Since rank is equal to the dimension of
       column (or row) span of the matrix, and the dimesnion of these spaces is
       determined by the number of pivots, it is thus impossible to have rank
       higher than four when adding $A$ and $B$.

*** Answer 12
    Let $C$ be arbitrary matrix of rank 2.  Since rank is preserved under
    multiplication with fully-ranked matrix, we can perform Gaussian elimination
    on $C$ to obtain a general-form matrix $C'$ lookin like this:
    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{align*}
        C' &= \begin{bmatrix}
          1      & \dots  & \dots \\
          0      & 1      & \dots \\
          \vdots & \vdots & \ddots
        \end{bmatrix}
      \end{align*}
    #+END_SRC
    Which, clearly, can be decomposed into two matrices of rank 1.  From
    distributivity of matrix multiplication over addition we can show that by
    multiplying by elementary matrices we can restore the obtained sum to match
    $C$ (the original matrix).  In other words: $C \times c_1E \times c_2E
    \times \dots \times c_nE = C'$, where $Es$ are some elementary matrices and
    $c_i$ are some constants, hence $(A + B) \times c_1E \times c_2E \times
    \dots \times c_nE = C$, provided $A + B = C'$.

** Problem 6
   Given bases $B = (\vec{u}_1, \vec{u}_2, \vec{u}_3)$ and $C = (\vec{v}_1,
   \vec{v}_2, \vec{v}_3)$ both in $\mathbb{R}^3$ s.t.
   #+HEADER: :exports results
   #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
   #+BEGIN_SRC latex
     \begin{align*}
       \vec{u}_1 &= (2,1,1) \\
       \vec{u}_2 &= (2,-1,1) \\
       \vec{u}_3 &= (1,2,1) \\
       \vec{v}_1 &= (3,1,-5) \\
       \vec{v}_2 &= (1,1,-3) \\
       \vec{v}_3 &= (-1,0,2)
     \end{align*}
   #+END_SRC
   1. Write the matrix of change of basis from $B$ to $C$ and its inverse.
   2. Compute the coordinate vector $[\vec{w}]_B$ where $\vec{w} = (-5,8,-5)$.
   3. Similarly, compute $[\vec{w}]_C$.

*** Answer 13
    The change of basis matrix from $B$ to $C$ can be found as follow:
    1. Undo the transformation from the standard basis to basis $B$ by
       multiplying the coordinates vector by $B^{-1}$.
    2. Multiply by $C$ to perform transformation.
    3. Multiply by $B$ to represent the resulting coordinates with respect to
       $B$.
       
    Or, in other words, $T_{B\to C} = C^{-1}BC$:
    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{align*}
        \begin{bmatrix}
          3  & 1  & -1 \\
          1  & 1  & 0 \\
          -5 & -3 & 2
        \end{bmatrix}^{-1} \times
        \begin{bmatrix}
          2 & 2  & 1 \\
          1 & -1 & 2 \\
          1 & 1  & 1
        \end{bmatrix} \times
        \begin{bmatrix}
          3  & 1  & -1 \\
          1  & 1  & 0 \\
          -5 & -3 & 2
        \end{bmatrix} = \\
        \begin{bmatrix}
          3  & 1  & -1 \\
          1  & 1  & 0 \\
          -5 & -3 & 2
        \end{bmatrix}^{-1} \times
        \begin{bmatrix}
          6 + 1 - 1 & 6 - 1 - 1 & 3 + 2 - 1 \\
          2 + 1 + 0 & 2 - 1 + 0 & 1 + 2 + 0 \\
          -10 - 3 + 2 & -10 + 3 + 2 & -5 - 6 + 3
        \end{bmatrix} = \\
        \begin{bmatrix}
          3  & 1  & -1 \\
          1  & 1  & 0 \\
          -5 & -3 & 2
        \end{bmatrix}^{-1} \times
        \begin{bmatrix}
          6 & 4 & 4 \\
          3 & 1 & 3 \\
          -11 & -5 & -8
        \end{bmatrix}
      \end{align*}
    #+END_SRC
    Now, let's find $C^{-1}$:
    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{align*}
        \begin{bmatrix}
          3  & 1  & -1 & 1 & 0 & 0 \\
          1  & 1  & 0  & 0 & 1 & 0 \\
          -5 & -3 & 2  & 0 & 0 & 1
        \end{bmatrix} \sim
        \begin{bmatrix}
          1 & 1  & 0  & 0 & 1  & 0 \\
          0 & -2 & -1 & 1 & -3 & 0 \\
          0 & 2  & 2  & 0 & 5  & 1
        \end{bmatrix} \sim \\
        \begin{bmatrix}
          1 & 1 & 0           & 0            & 1           & 0 \\
          0 & 1 & \frac{1}{2} & -\frac{1}{2} & \frac{3}{2} & 0 \\
          0 & 0 & 1           & 1            & 2           & 1
        \end{bmatrix} \sim 
        \begin{bmatrix}
          1 & 0 & 0 & 1  & -\frac{1}{2} & \frac{1}{2} \\
          0 & 1 & 0 & -1 & \frac{1}{2}  & -\frac{1}{2} \\
          0 & 0 & 1 & 1  & 2            & 1
        \end{bmatrix}
      \end{align*}
    #+END_SRC
    finally:
    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{align*}
        \begin{bmatrix}
          3  & 1  & -1 \\
          1  & 1  & 0 \\
          -5 & -3 & 2
        \end{bmatrix}^{-1} \times
        \begin{bmatrix}
          6   & 4  & 4 \\
          3   & 1  & 3 \\
          -11 & -5 & -8
        \end{bmatrix} = 
        \begin{bmatrix}
          1  & -\frac{1}{2} & \frac{1}{2} \\
          -1 & \frac{1}{2}  & -\frac{1}{2} \\
          1  & 2            & 1
        \end{bmatrix} \times
        \begin{bmatrix}
          6   & 4  & 4 \\
          3   & 1  & 3 \\
          -11 & -5 & -8
        \end{bmatrix} = \\
        \begin{bmatrix}
          6 - \frac{3}{2} - \frac{11}{2}  & 4 - \frac{1}{2} - \frac{5}{2}  & 4 - \frac{3}{2} - 4 \\
          -6 + \frac{3}{2} + \frac{11}{2} & -4 + \frac{1}{2} + \frac{5}{2} & -4 + \frac{3}{2} + 4 \\
          6 + 6 - 11                      & 4 + 2 - 5                      & 4 + 6 - 8
        \end{bmatrix} = 
        \begin{bmatrix}
          -1 & 1 & \frac{3}{2} \\
          1  & 1 & \frac{3}{2} \\
          1  & 1 & 2
        \end{bmatrix}
      \end{align*}
    #+END_SRC

*** Answer 14
    $[\vec{w}]_B$ is just $B^{-1}w$.  First, compute $B^{-1}$:
    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{align*}
        \begin{bmatrix}
          2 & 2  & 1 & 1 & 0 & 0 \\
          1 & -1 & 2 & 0 & 1 & 0 \\
          1 & 1  & 1 & 0 & 0 & 1
        \end{bmatrix} \sim
        \begin{bmatrix}
          1 & 1  & 1  & 0 & 0 & 1 \\
          0 & -2 & 1  & 0 & 1 & -1 \\
          0 & 0  & -1 & 1 & 0 & -2
        \end{bmatrix} \sim
        \begin{bmatrix}
          1 & 1 & 1            & 0  & 0            & 1 \\
          0 & 1 & -\frac{1}{2} & 0  & -\frac{1}{2} & \frac{1}{2} \\
          0 & 0 & 1            & -1 & 0            & 2
        \end{bmatrix} \sim \\
        \begin{bmatrix}
          1 & 1 & 0 & 1            & 0            & -1 \\
          0 & 1 & 0 & -\frac{1}{2} & -\frac{1}{2} & \frac{3}{2} \\
          0 & 0 & 1 & -1           & 0            & 2
        \end{bmatrix} \sim
        \begin{bmatrix}
          1 & 0 & 0 & \frac{3}{2}  & \frac{1}{2}  & -\frac{5}{2} \\
          0 & 1 & 0 & -\frac{1}{2} & -\frac{1}{2} & \frac{3}{2} \\
          0 & 0 & 1 & -1           & 0            & 2
        \end{bmatrix} \\
        \textit{Hence }B^{-1} = 
        \begin{bmatrix}
          \frac{3}{2}  & \frac{1}{2}  & -\frac{5}{2} \\
          -\frac{1}{2} & -\frac{1}{2} & \frac{3}{2} \\
          -1           & 0            & 2
        \end{bmatrix}
      \end{align*}
    #+END_SRC
    Thus, $[\vec{w}]_B$ can be computed via:
    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{align*}
        \begin{bmatrix}
          \frac{3}{2}  & \frac{1}{2}  & -\frac{5}{2} \\
          -\frac{1}{2} & -\frac{1}{2} & \frac{3}{2} \\
          -1           & 0            & 2
        \end{bmatrix} \times 
        \begin{bmatrix}
          -5 \\
          8 \\
          -5
        \end{bmatrix} =
        \begin{bmatrix}
          -\frac{15}{2} + 4 + \frac{25}{2} \\
          \frac{5}{2} - 4 - \frac{15}{2} \\
          5 - 10
        \end{bmatrix} = 
        \begin{bmatrix}
          9 \\
          1 \\
          -5
        \end{bmatrix}
      \end{align*}
    #+END_SRC

*** Answer 15
    Similarly, $[\vec{w}]_C$ is just $C^{-1}w$ (we've already computed $C^{-1}$
    in the previous answer.)
    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{align*}
        \begin{bmatrix}
          1  & -\frac{1}{2} & \frac{1}{2} \\
          -1 & \frac{1}{2}  & -\frac{1}{2} \\
          1  & 2            & 1
        \end{bmatrix} \times
        \begin{bmatrix}
          -5 \\
          8 \\
          -5
        \end{bmatrix} =
        \begin{bmatrix}
          -5 - 4 - \frac{5}{2} \\
          5  + 4 + \frac{5}{2} \\
          -5 + 16 - 5
        \end{bmatrix} =
        \begin{bmatrix}
          -\frac{23}{2} \\
          \frac{23}{2} \\
          6
        \end{bmatrix}
      \end{align*}
    #+END_SRC
