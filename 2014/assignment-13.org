# -*- fill-column: 80; org-confirm-babel-evaluate: nil -*-

#+TITLE:     Assignment 13, Linear Algebra 1
#+AUTHOR:    Oleg Sivokon
#+EMAIL:     olegsivokon@gmail.com
#+DATE:      <2014-12-12 Fri>
#+DESCRIPTION: Third asssignment in the course Linear Algebra 1
#+KEYWORDS: Assignment, Linear Algebra
#+LANGUAGE: en
#+LaTeX_CLASS: article
#+LATEX_HEADER: \usepackage[usenames,dvipsnames]{color}
#+LATEX_HEADER: \usepackage{a4wide}
#+LATEX_HEADER: \usepackage[backend=bibtex, style=numeric]{biblatex}
#+LATEX_HEADER: \usepackage{commath}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usetikzlibrary{shapes,backgrounds}
#+LATEX_HEADER: \usepackage{marginnote}
#+LATEX_HEADER: \usepackage{enumerate}
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \hypersetup{urlcolor=blue}
#+LATEX_HEADER: \hypersetup{colorlinks,urlcolor=blue}
#+LATEX_HEADER: \addbibresource{bibliography.bib}
#+LATEX_HEADER: \setlength{\parskip}{16pt plus 2pt minus 2pt}
#+LATEX_HEADER: \definecolor{codebg}{rgb}{0.96,0.99,0.8}

#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="./css/style.css"/>
#+HTML_HEAD_EXTRA: <link rel="stylesheet" type="text/css" href="./css/bootstrap.min.css"/>
#+HTML_HEAD_EXTRA: <link rel="stylesheet" type="text/css" href="./css/icfp.css"/>

#+BEGIN_SRC emacs-lisp :exports none
(setq org-latex-pdf-process
        '("latexmk -pdflatex='pdflatex -shell-escape -interaction nonstopmode' -pdf -bibtex -f %f")
        org-latex-listings t
        org-src-fontify-natively t
        org-babel-latex-htlatex "htlatex")
(defmacro by-backend (&rest body)
    `(cl-case (when (boundp 'backend) (org-export-backend-name backend))
       ,@body))
#+END_SRC

#+RESULTS:
: by-backend

#+BEGIN_LATEX
  \lstset{ %
    backgroundcolor=\color{codebg},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
    breaklines=false,
    captionpos=b,                    % sets the caption-position to bottom
    commentstyle=\color{mygreen},    % comment style
    framexleftmargin=10pt,
    xleftmargin=10pt,
    framerule=0pt,
    frame=tb,                        % adds a frame around the code
    keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
    keywordstyle=\color{blue},       % keyword style
    showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
    showstringspaces=false,          % underline spaces within strings only
    showtabs=false,                  % show tabs within strings adding particular underscores
    stringstyle=\color{codestr},     % string literal style
    tabsize=2,                       % sets default tabsize to 2 spaces
  }
#+END_LATEX

@@latex: \clearpage@@

* Problems

** Problem 1

   1. Given:
      
      #+HEADER: :exports results
      #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
      #+BEGIN_SRC latex
        \begin{equation*}
          \begin{split}
            w = 1 - i \\
            t = \cos{\frac{3 \pi}{4}} + i \sin{\frac{3 \pi}{4}}
          \end{split}
        \end{equation*}
      #+END_SRC
   
      Solve:
   
      #+HEADER: :exports results
      #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
      #+BEGIN_SRC latex
        \begin{equation*}
          z^3 = \frac{w}{\bar{t}}
        \end{equation*}
      #+END_SRC
      
   2. Let $z_1, z_2, ..., z_n$ be all solutions of an equation $z^n=1$ in $\mathbb{C}$.
      
      Prove that $z_1 \times z_2 \times ... \times z_n = 1$.
        
*** Answer 1
    Let's first represent $w$ in polar form:

    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{align*}
        w            &= re^{\theta i} \\
        r            &= \sqrt{1^2 - i^2} = \sqrt{1 + 1} = \sqrt{2} \\
        \tan{\theta} &= \frac{-1}{1} \\
        \theta       &= \tan^{-1}{-1} = \frac{\pi}{4} \\
        w            &= \sqrt{2}e^{\frac{\pi}{4}}
      \end{align*}
    #+END_SRC

    Next, substitute both $w$ and $\bar{t}$ into equation:

    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{align*}
        z &= \Big(\frac{\sqrt{2}e^{\frac{i\pi}{4}}}{e^{\frac{-i4\pi}{3}}}\Big)^{\frac{1}{3}} \\
          &= \big(\sqrt{2}e^{\frac{i\pi - (-3i\pi)}{4}}\big)^{\frac{1}{3}} \\
          &= (\sqrt{2}e^{\pi})^{\frac{1}{3}} \\
          &= \sqrt[2\times 3]{2}e^{i\pi \times \frac{1}{3}} \\
          &= \sqrt[6]{2}e^{\frac{i\pi}{3}}
      \end{align*}
    #+END_SRC

    Which is the reduced polar form.

    Now, let's verify:

    #+HEADER: :exports both
    #+BEGIN_SRC lisp
      (expt
       (/ (- 1 #c(0 -1))
          (- (cos (/ (* 3 pi) 4))
             (* #c(0 1) (sin (/ (* 3 pi) 4)))))
            1/3)
    #+END_SRC

    #+HEADER: :exports both
    #+BEGIN_SRC lisp
      (* (expt 2 1/6) (expt (exp 1) (* #c(0 1) (/ pi 3))))
    #+END_SRC

    By fundamental theorem of algebra, this equation must have two more results.
    So, the general solution to this equation must be given by

    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{align*}
        z &= \sqrt[6]{2}e^{\frac{i\pi+2\pi K}{3}}
      \end{align*}
    #+END_SRC

    Where $k=1,2,3$. /(this is probably wrong)/

*** Answer 2
    Unfortunately, the conjecture is false, thus no proof is possible.  For example,
    for the case $n=2$, it gives $z^2=1$ with two roots: $z=1$ and $z=-1$, where
    $1 \times (-1) = -1$.

    However, with some refinements, it is possible to prove this conjecture for
    all non-real roots, to do so let $a_i$, $i=1...n$ be the solution of $z^n-1=0$.
    Then:

    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{equation*}
        z - 1^n = (z - a_1)(z - a_2)...(z - a_n)
      \end{equation*}
    #+END_SRC

    After we open parenthesis and multiply the terms, we will get:

    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{equation*}
        (-1)^n a_1...a_n
      \end{equation*}
    #+END_SRC

    When we equate the coefficients, we get that:

    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{equation*}
        a_1...a_n = (-1)^{n+1}
      \end{equation*}
    #+END_SRC

    Now, observe that one $n$ is odd, then the only real solution is 1, then
    the rest of the solutions would give us $(-1)^{n+1} = 1$.  And when $n$
    is even, we will have two real solutions: 1 and $-1$. Then the product of
    all non-real solutions would be: $\frac{(-1)^{n+1}}{-1} = 1$.

** Problem 2
   Given $A = \{(x,1)|x \in \mathbb{R}\}$ is a subset of $\mathbb{R}^2$ and
   operations over $A$ defined as follows:

   + Addition, $\oplus$: 
     $\forall x, y \in \mathbb{R}: (x,1)\oplus(y,1) = (x+y,1)$.
   + Multiplication by scalar, $\odot$:
     $\forall k, y \in \mathbb{R}: k \odot (x,1) = (k^2x,1)$.
   + Multiplication, $*$:
     $\forall x, y \in \mathbb{R}: (x,1)*(y,1) = (x*y,1)$.


   1. Is $(A, \oplus, \odot)$ a vector space over $\mathbb{R}$?
   2. Is $(A, \oplus, *)$ a field?

*** Answer 3
    No, $(A, \oplus, \odot)$ is not a vector space over $\mathbb{R}$.  One of
    requirements of a vector field is that multiplication with scalar give
    a /unique/ solution in $A$, while every pair of any member of $A$ and
    its additive inverse violates this requirement, for instance:

    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{equation*}
        -1\odot(x,1)=((-1)^2x,1)=(x,1)=(1^2x,1)=1\odot(x,1)
      \end{equation*}
    #+END_SRC

    1. Vector addition is commutative:
       #+HEADER: :exports results
       #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
       #+BEGIN_SRC latex
         \begin{equation*}
           (x,1)+(y,1)=(x+y,1)=(y+x,1)=(y,1)+(x,1)
         \end{equation*}
       #+END_SRC
       
    2. Vector addition is associative:
       #+HEADER: :exports results
       #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
       #+BEGIN_SRC latex
         \begin{equation*}
           (x,1)+((y,1)+(z,1))=(x,1)+(y+z,1)=(x+y+z,1)=(x+y,1)+(z,1)
         \end{equation*}
       #+END_SRC
       
    3. There exists additive identity $0$:
       As the previous two, this follows from addition in $\mathbb{R}$ having
       additive identity.
    
    4. 1 is the multiplicative idenity:
       Since $1^2=1$, we can reuse the multiplicative identity defined on reals.
       
    5. Every element has an additive inverse:
       Again, we are using the addition in real numbers, so we are guaranteed to
       have additive inverses for every such number.
       
    6. Scalar multiplication is associative:
       #+HEADER: :exports results
       #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
       #+BEGIN_SRC latex
         \begin{equation*}
           r(k\odot(x,1))=r(k^2x,1)=(r^2k^2x,1)=((rk)^2x,1)=(rk)(x,1)
         \end{equation*}
       #+END_SRC
       
    7. Scalar multiplication is distributive:
       #+HEADER: :exports results
       #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
       #+BEGIN_SRC latex
         \begin{equation*}
           k\odot((x,1)+(y,1))=k\odot(x+y,1)=(k^2(x+y),1)=(k^2x,1)+(k^2y,1)=k\odot(x,1)+k\odot(y,1)
         \end{equation*}
       #+END_SRC

** Problem 3
   1. Establish which of the given sets are linear vector fields over $\mathbb{F}$
      under normal /(what is considered ``normal'' addition of tuples of complex/
      /numbers?)/ operations.

      + $U = \{ (z, w) \in \mathbb{C}^2 \; | \; 2z = 3w \}$, $\mathbb{F} = \mathbb{C}$.
      + $W = \{ f : \mathbb{R} \to \mathbb{R} \; | \; f(x + 1) = f(x) + 1, x \in \mathbb{R} \}$,
        $\mathbb{F} = \mathbb{R}$.
      + $M = \{ p(x) \in \mathbb{R}^4[x] \; | \; p(x) = p(x - 1) \}$, 
        $\mathbb{F} = \mathbb{R}$.
      + $S = \Bigg \{ \begin{bmatrix}a &b \\ c &d\end{bmatrix} \in M_2(\mathbb{R}) \; | \; ad = 0 \Bigg \}$,
        $\mathbb{F} = \mathbb{R}$.
     
   2. For every vector space found, write its finite spanning set.

*** Answer 4
    $U$ is a vector space, assuming addition is point-wise.  Addition properties
    follow from the same properties of addition of complex numbers.  Similarly
    for multiplication with field's identity element and multiplicatin by scalar.

    $W$ is certainly not a vector space.  Put $f(0) = -3$, then $f(1) = -2$ and
    $f(2) = -1$.  Now $4 \times f(1 + 2) = 4 \times f(3) = 4 \times 0 = 0$, while
    $4 \times f(1) + 4 \times f(2) = 4 \times -2 + 4 \times -1 = -8 - 4 = -12$.
    Clearly $-12 \neq 0$.

    $M$ is a vector space since.  To see this let's first eamine what kinds of
    polinomials are represented by $W$:

    First, let's find the zero polinomial.  Looking at $p(1)$ would be also
    interesting because it will immediately show what kinds of polinomials are
    possible in this field.

    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{equation*}
        \begin{split}
          p(0)=\alpha_0 + \alpha_1 \times 0 + \alpha_2 \times 0^2 + \alpha_2 \times 0^3 \\
          p(1)=\alpha_0 + \alpha_1 \times 1 + \alpha_2 \times 1^2 + \alpha_2 \times 1^3
        \end{split}
      \end{equation*}
    #+END_SRC
    
    Which, in turn implies that since $p(1)=p(1-1)=p(0)$, only the constant term of
    these two polinomials matters, in other words, $p(0)=\alpha_0=p(1)$ implies
    that other terms of these polinomials must be zero.

    This leaves us with polinomials of the form $p(x)=\alpha_0$, adding such
    polinomials will be equivalent to addition defined for real numbers, same goes
    for multiplication.
    
    $S$ is not a vector space.  Adding any two matrices where $a \neq d$ and $a' \neq a$
    would give us a matrix where both $a$ and $d$ are non-zero, producing a matrix
    outside the valid range.

*** Answer 5
    
    1. Example spanning set for $U$ would be $Sp(i, \frac{3}{2}i)$.
    2. Example spanning set for $M$ would be $Sp(p(x)=1)$.

** Problem 4
   1. Let $\vec{u}$, $\vec{v}$ and $\vec{w}$ be vectors in linear vector field $V$
      over $\mathbb{F}$.  Does it then hold that:
      $Sp \{u, v, w\} = Sp \{u + v - w, u - v + 2w, v + w\}$?
   2. Let $U = Sp \{ (1, 2, 5), (1, 1, 3)\}$ and $W = Sp \{ (1, 0, 1), (0, 1, 1)\}$.
      Are $U = W$?

*** Answer 6
    Let's equate two expressions and try to see if equality holds:
    
    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{tabular}{lll}
        u+v+w&=&\lambda_0(u+v-w)+\lambda_1(u-v+2w)+\lambda_2(v+w) \\
        &=&u(\lambda_0+\lambda_1)+v(\lambda_0-\lambda_1+\lambda_2)+
        w(-\lambda_0+2\lambda_1+\lambda_2)
      \end{tabular}
    #+END_SRC
    From here we can already see that we can make coefficients arbitrary large.
    In other words, the coefficients of the vectors are linearly independant:

    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{equation*}
        \begin{bmatrix}
          1 & 1 & -1 \\
          1 & -1 & 2 \\
          0 & 1 & 1 \\
        \end{bmatrix}
        \to
        \begin{bmatrix}
          1 & 1 & -1 \\
          0 & 1 & 1 \\
          1 & -1 & 2 \\
        \end{bmatrix}
        \to
        \begin{bmatrix}
          1 & 1 & -1 \\
          0 & 1 & 1 \\
          0 & -2 & 3 \\
        \end{bmatrix}
        \to
        \begin{bmatrix}
          1 & 1 & -1 \\
          0 & 1 & 1 \\
          0 & 0 & 5 \\
        \end{bmatrix}
      \end{equation*}
   #+END_SRC

    Since every column of this matrix has a pivot, the vectors represented by
    its columns are linearly independent.  Hence, both spans are quivalent.

*** Answer 7
    Assuming equality of spanning sets means that its the spaces spanned by them
    must be equal and not the spanning sets themselves (otherwise the answer
    would be obviously negative).  In order to find out whether the span of the
    space is the same, we could adjoin a vecotr from one set to the vectors from
    another set and see if we obtain linearly dependant set.  If the set is
    linearly dependent, it would mean that the vector from another set may be
    generated from the first set, and if this is true for all vectors in the
    other set, then it would mean that two spans are the same.  However, knowing
    that a vector from a spanning set cannot be generated by applying elementary
    operations to the set of vectors, we would know that it is not possible to
    generate it using the spanning set, thus they must be different.

    We will adjoin (1, 0, 1), (0, 1, 1) and (1, 1, 3):

    #+HEADER: :exports results
    #+HEADER: :results (by-backend (pdf "latex") (t "raw"))
    #+BEGIN_SRC latex
      \begin{equation*}
        \begin{bmatrix}
          1 & 0 & 1 \\
          0 & 1 & 1 \\
          1 & 1 & 3 \\
        \end{bmatrix}
        \begin{aligned} \to \end{aligned}
        \begin{bmatrix}
          1 & 0 & 1 \\
          0 & 1 & 1 \\
          0 & 1 & 2 \\
        \end{bmatrix}
        \begin{aligned} \to \end{aligned}
        \begin{bmatrix}
          1 & 0 & 1 \\
          0 & 1 & 1 \\
          0 & 0 & 1 \\
        \end{bmatrix}
      \end{equation*}
    #+END_SRC

    Since the rank of this matrix is 3, it represents a linearly independent
    combination of vectors, hence $U \neq W$.

** Problem 5
   Let $U$ and $W$ be sub-spaces of the linear vector space $V$ s.t. $U \oplus W = V$.
   Let $S \subseteq U$ and $T \subseteq W$ be two finite linearly independent sets.
   Prove that $S \cup T$ is linearly independent.

*** Answer 8
    Since we know that $S$ and $T$ are both linearly independant, they are also spans,
    they either span the entire subspace, from which they are taken, or a subspace of
    that subspace.  Now, suppose their union was linearly dependant, this, would also
    imply that the subspaces from which they were taken had common members other than
    the zero vector (those would be exactly the vectors that must have been common
    to the union of $S$ and $T$.  Since by the definition of direct sum this is not
    possible, it must be that union of $S$ and $T$ is linearly independant.
